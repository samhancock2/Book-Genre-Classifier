glove 6B 50 dim model: 71% acc
glove 6B 100 dim model: 72% acc
glove 6B 200 dim model: 75% acc
glove 6B 300 dim model: 76% acc 
epochs = 50 , bs = 32, lr = 0.001, hd = 128, patience = 3



hyperparam tuning (grid search) on glove models: (ran twice: patience=3 - 76.9% best acc, patience = 7 - 77.3% best acc)
glove 6B 300 dim model: 76.1% acc --> 77.32% acc 
epochs = 50, bs = 64, lr = 0.0005, hd = 128, patience = 7


sentence-transformers 384 dim model 76% acc

hyperparam tuning (grid search) on sentence-transformers model: (patience = 3)
Accuracy: 0.7766
Params: {'lr': 0.002, 'batch_size': 64, 'hidden_dim': 256, 'epochs': 50, 'warmup_ratio': 0.1}


GRU initial: 
    Features:
        Truncated to 300 words per blurb
        Used GloVe 300 dim embeddings
        Padding
        Mean pooling
        No Bidirectionality
        patience = 3
    Test Accuracy: 0.7676
    Overfitting from epoch 1

GRU experiments:
    1. Dropout (0.5) + weight decay (1e-5) + Bidirectionality + hidden_dim 128 -> 64 + patience = 5:
    Result: Much less overfitting, test accuracy ~ 0.74 - 0.76
    
    2. Attention pooling (instead of mean):
    Result: test accuracy 0.7531

    3. Truncated blurbs to 600 words instead of 300:
    Result: test accuracy 0.7630
    Truncated blurbs to 150 words:
    Result: test accuracy 0.7543
    

DistilBERT:
    Embedding dim: 768
    Max token length: 256
    Epochs: 3
    LR: 2e-5
    Weight decay: 0.01
    Test accuracy: 0.8222
    
 

To Do:
Balance classes

image/NLP similarity project