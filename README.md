# ğŸ§  Text Classification with Traditional and Transformer-Based Embeddings

## ğŸ“˜ Project Overview
This project benchmarks multiple natural language processing (NLP) approaches for **text classification** on a dataset of ~34â€¯k items (product or text descriptions).  
The objective was to compare traditional embedding-based and sequence models against modern transformer-based architectures.

Each model was trained on the same preprocessed dataset and evaluated on test accuracy.

---

## ğŸ§¾ Model Performance Summary

| Model Type | Configuration Highlights | Test Accuracy |
|-------------|--------------------------|----------------|
| **GloVeâ€¯50D** | 50-dimensional GloVe (6B) | **71â€¯%** |
| **GloVeâ€¯100D** | 100-dimensional GloVe (6B) | **72â€¯%** |
| **GloVeâ€¯200D** | 200-dimensional GloVe (6B) | **75â€¯%** |
| **GloVeâ€¯300D** | 300-dimensional GloVe (6B) | **76â€¯%** |
| **Sentenceâ€‘Transformer** | 384-dimensional sentence vectors (`allâ€‘MiniLMâ€‘L6â€‘v2`) | **76â€¯%** |
| **GRU (optimized)** | Biâ€‘GRUâ€¯+â€¯dropoutâ€¯+â€¯attention, 300Dâ€¯GloVeâ€¯inputs | **0.74â€¯â€“â€¯0.77** |
| **DistilBERTâ€¯(Base)** | Transformer fineâ€‘tuned on task | **82.2â€¯% â­â€¯(best)** |

---

## âš™ï¸ Experimental Details

### ğŸ”¹ GloVe Models
- Used pre-trained **GloVeâ€¯6B** embeddings of 50â€“300 dimensions.  
- Classifier: feed-forward network (hidden_dimâ€¯=â€¯128).  
- **Training:** 50â€¯epochs, batchâ€¯sizeâ€¯32, learning_rateâ€¯0.001, patienceâ€¯3.  
- Performance improved steadily with embedding size (plateaued around 300â€¯D).

**Hyperparameter Search**
- Ran grid search twice:  
  - Patienceâ€¯=â€¯3â€¯â†’â€¯bestâ€¯76.9â€¯%  
  - Patienceâ€¯=â€¯7â€¯â†’â€¯bestâ€¯77.3â€¯%  
- Best config: `epochsâ€¯=â€¯50`,â€¯`batchâ€¯=â€¯64`,â€¯`lrâ€¯=â€¯5eâ€‘4`,â€¯`hidden_dimâ€¯=â€¯128`.

---

### ğŸ”¹ Sentenceâ€‘Transformer Models
- Base model: **`sentence-transformers/allâ€‘MiniLMâ€‘L6â€‘v2`** (384-dimensional sentence vectors).  
- **Training:** epochsâ€¯=â€¯50, batchâ€¯sizeâ€¯64, learning_rateâ€¯=â€¯0.002, hidden_dimâ€¯=â€¯256, warmupâ€¯ratioâ€¯=â€¯0.1, patienceâ€¯=â€¯3.  
- **Accuracy:**â€¯77.7â€¯%.  
- Provided strong semantic embeddings but slightly below transformer fineâ€‘tuning.

---

### ğŸ”¹ GRU Models
**Initial setup**
- Truncated texts toâ€¯300â€¯words per example.  
- Input embeddings:â€¯GloVeâ€¯300D.  
- Paddingâ€¯+â€¯mean pooling; nonâ€‘bidirectional GRU; patienceâ€¯=â€¯3.  
- Testâ€¯accuracyâ€¯=â€¯0.7676â€¯with early overfitting.

**Further Experiments**
1. Dropoutâ€¯0.5â€¯+â€¯weight_decayâ€¯1eâ€‘5â€¯+â€¯bidirectionalâ€¯GRUâ€¯+â€¯hidden_dimâ€¯128â€¯â†’â€¯64â€¯+â€¯patienceâ€¯=â€¯5  
   â†’â€¯Reduced overfitting,â€¯accuracyâ€¯â‰ˆâ€¯0.74â€¯â€“â€¯0.76.  
2. Attention pooling instead of mean â†’â€¯accuracyâ€¯â‰ˆâ€¯0.753.  
3. Sequence length tests (150â€¯/â€¯300â€¯/â€¯600â€¯tokens):  
   - 600â€¯=â€¯0.763,â€¯150â€¯=â€¯0.754.

---

### ğŸ”¹ DistilBERT (Transformers)
**Model Configuration**
- Base model:â€¯`distilbert-base-uncased`.  
- Embeddingâ€¯dimâ€¯=â€¯768, 6â€¯transformerâ€¯layers, WordPieceâ€¯vocabâ€¯â‰ˆâ€¯30â€¯k.  
- Maxâ€¯sequenceâ€¯lengthâ€¯=â€¯256â€¯tokensâ€¯(â‰ˆâ€¯200â€¯words).  
- Fineâ€‘tuned with classification head (softmaxâ€¯overâ€¯labels).  

**Training Arguments**