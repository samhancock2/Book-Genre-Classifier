{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc7d6fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce GTX 1660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9174b4bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (4.56.2)\n",
      "Requirement already satisfied: datasets in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (4.2.0)\n",
      "Requirement already satisfied: accelerate in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (1.10.1)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from httpx<1.0.0->datasets) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from accelerate) (2.8.0+cu129)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets accelerate evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d570cfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# === Load the same CSV ===\n",
    "file_path = \"../data/processed/full_2k.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# encode labels the same way\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['Category'])\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# reproducible splits (same as before)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    df['Description'], df['label'],\n",
    "    test_size=0.1, stratify=df['label'], random_state=13\n",
    ")\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp, y_temp,\n",
    "    test_size=0.1111111,\n",
    "    stratify=y_temp, random_state=13\n",
    ")\n",
    "\n",
    "# create small dataframes\n",
    "train_df = pd.DataFrame({'text': X_train, 'label': y_train})\n",
    "val_df   = pd.DataFrame({'text': X_val,   'label': y_val})\n",
    "test_df  = pd.DataFrame({'text': X_test,  'label': y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48c0b50d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1704bbcfbc84f44a1810cf78e3421dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27543 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e57f381485d94b73a8d781977d557c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acaa187606548d6bb9649e2bfd1a4f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3443 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch[\"text\"],\n",
    "                     truncation=True,\n",
    "                     padding=\"max_length\",\n",
    "                     max_length=256)\n",
    "\n",
    "# Convert to HF Dataset\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "val_ds   = Dataset.from_pandas(val_df)\n",
    "test_ds  = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Map tokenizer over data\n",
    "train_ds = train_ds.map(tokenize, batched=True, batch_size=len(train_ds))\n",
    "val_ds   = val_ds.map(tokenize, batched=True, batch_size=len(val_ds))\n",
    "test_ds  = test_ds.map(tokenize, batched=True, batch_size=len(test_ds))\n",
    "\n",
    "# Set the correct tensor columns\n",
    "train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "val_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59a10ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c65ef252",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\the3s\\AppData\\Local\\Temp\\ipykernel_11864\\100774169.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from evaluate import load\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    return {\"accuracy\": metric.compute(predictions=preds, references=labels)[\"accuracy\"]}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_runs\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9e62aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12' max='8610' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  12/8610 00:05 < 1:20:56, 1.77 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m results = trainer.evaluate(test_ds)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[33m'\u001b[39m\u001b[33meval_accuracy\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\the3s\\AnacondaInstall\\envs\\glove-env\\Lib\\site-packages\\transformers\\trainer.py:2328\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[32m   2326\u001b[39m         args=args,\n\u001b[32m   2327\u001b[39m         resume_from_checkpoint=resume_from_checkpoint,\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m         trial=trial,\n\u001b[32m   2329\u001b[39m         ignore_keys_for_eval=ignore_keys_for_eval,\n\u001b[32m   2330\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\the3s\\AnacondaInstall\\envs\\glove-env\\Lib\\site-packages\\transformers\\trainer.py:2677\u001b[39m, in \u001b[36m_inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m   2674\u001b[39m     tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs, num_items_in_batch)\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n\u001b[32m   2683\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "\n",
    "results = trainer.evaluate(test_ds)\n",
    "print(f\"Test accuracy: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f0f7cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e6e7ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (4.56.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.0-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\the3s\\anacondainstall\\envs\\glove-env\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Downloading transformers-4.57.0-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 5.2/12.0 MB 39.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 39.6 MB/s  0:00:00\n",
      "Installing collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.56.2\n",
      "    Uninstalling transformers-4.56.2:\n",
      "      Successfully uninstalled transformers-4.56.2\n",
      "Successfully installed transformers-4.57.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b7e4864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TrainingArguments in module transformers.training_args:\n",
      "\n",
      "class TrainingArguments(builtins.object)\n",
      " |  TrainingArguments(\n",
      " |      output_dir: Optional[str] = None,\n",
      " |      overwrite_output_dir: bool = False,\n",
      " |      do_train: bool = False,\n",
      " |      do_eval: bool = False,\n",
      " |      do_predict: bool = False,\n",
      " |      eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no',\n",
      " |      prediction_loss_only: bool = False,\n",
      " |      per_device_train_batch_size: int = 8,\n",
      " |      per_device_eval_batch_size: int = 8,\n",
      " |      per_gpu_train_batch_size: Optional[int] = None,\n",
      " |      per_gpu_eval_batch_size: Optional[int] = None,\n",
      " |      gradient_accumulation_steps: int = 1,\n",
      " |      eval_accumulation_steps: Optional[int] = None,\n",
      " |      eval_delay: Optional[float] = 0,\n",
      " |      torch_empty_cache_steps: Optional[int] = None,\n",
      " |      learning_rate: float = 5e-05,\n",
      " |      weight_decay: float = 0.0,\n",
      " |      adam_beta1: float = 0.9,\n",
      " |      adam_beta2: float = 0.999,\n",
      " |      adam_epsilon: float = 1e-08,\n",
      " |      max_grad_norm: float = 1.0,\n",
      " |      num_train_epochs: float = 3.0,\n",
      " |      max_steps: int = -1,\n",
      " |      lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear',\n",
      " |      lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>,\n",
      " |      warmup_ratio: float = 0.0,\n",
      " |      warmup_steps: int = 0,\n",
      " |      log_level: str = 'passive',\n",
      " |      log_level_replica: str = 'warning',\n",
      " |      log_on_each_node: bool = True,\n",
      " |      logging_dir: Optional[str] = None,\n",
      " |      logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps',\n",
      " |      logging_first_step: bool = False,\n",
      " |      logging_steps: float = 500,\n",
      " |      logging_nan_inf_filter: bool = True,\n",
      " |      save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps',\n",
      " |      save_steps: float = 500,\n",
      " |      save_total_limit: Optional[int] = None,\n",
      " |      save_safetensors: Optional[bool] = True,\n",
      " |      save_on_each_node: bool = False,\n",
      " |      save_only_model: bool = False,\n",
      " |      restore_callback_states_from_checkpoint: bool = False,\n",
      " |      no_cuda: bool = False,\n",
      " |      use_cpu: bool = False,\n",
      " |      use_mps_device: bool = False,\n",
      " |      seed: int = 42,\n",
      " |      data_seed: Optional[int] = None,\n",
      " |      jit_mode_eval: bool = False,\n",
      " |      use_ipex: bool = False,\n",
      " |      bf16: bool = False,\n",
      " |      fp16: bool = False,\n",
      " |      fp16_opt_level: str = 'O1',\n",
      " |      half_precision_backend: str = 'auto',\n",
      " |      bf16_full_eval: bool = False,\n",
      " |      fp16_full_eval: bool = False,\n",
      " |      tf32: Optional[bool] = None,\n",
      " |      local_rank: int = -1,\n",
      " |      ddp_backend: Optional[str] = None,\n",
      " |      tpu_num_cores: Optional[int] = None,\n",
      " |      tpu_metrics_debug: bool = False,\n",
      " |      debug: Union[str, list[transformers.debug_utils.DebugOption]] = '',\n",
      " |      dataloader_drop_last: bool = False,\n",
      " |      eval_steps: Optional[float] = None,\n",
      " |      dataloader_num_workers: int = 0,\n",
      " |      dataloader_prefetch_factor: Optional[int] = None,\n",
      " |      past_index: int = -1,\n",
      " |      run_name: Optional[str] = None,\n",
      " |      disable_tqdm: Optional[bool] = None,\n",
      " |      remove_unused_columns: Optional[bool] = True,\n",
      " |      label_names: Optional[list[str]] = None,\n",
      " |      load_best_model_at_end: Optional[bool] = False,\n",
      " |      metric_for_best_model: Optional[str] = None,\n",
      " |      greater_is_better: Optional[bool] = None,\n",
      " |      ignore_data_skip: bool = False,\n",
      " |      fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '',\n",
      " |      fsdp_min_num_params: int = 0,\n",
      " |      fsdp_config: Union[dict[str, Any], str, NoneType] = None,\n",
      " |      fsdp_transformer_layer_cls_to_wrap: Optional[str] = None,\n",
      " |      accelerator_config: Union[dict, str, NoneType] = None,\n",
      " |      parallelism_config: Optional[ForwardRef('ParallelismConfig')] = None,\n",
      " |      deepspeed: Union[dict, str, NoneType] = None,\n",
      " |      label_smoothing_factor: float = 0.0,\n",
      " |      optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused',\n",
      " |      optim_args: Optional[str] = None,\n",
      " |      adafactor: bool = False,\n",
      " |      group_by_length: bool = False,\n",
      " |      length_column_name: Optional[str] = 'length',\n",
      " |      report_to: Union[NoneType, str, list[str]] = None,\n",
      " |      ddp_find_unused_parameters: Optional[bool] = None,\n",
      " |      ddp_bucket_cap_mb: Optional[int] = None,\n",
      " |      ddp_broadcast_buffers: Optional[bool] = None,\n",
      " |      dataloader_pin_memory: bool = True,\n",
      " |      dataloader_persistent_workers: bool = False,\n",
      " |      skip_memory_metrics: bool = True,\n",
      " |      use_legacy_prediction_loop: bool = False,\n",
      " |      push_to_hub: bool = False,\n",
      " |      resume_from_checkpoint: Optional[str] = None,\n",
      " |      hub_model_id: Optional[str] = None,\n",
      " |      hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save',\n",
      " |      hub_token: Optional[str] = None,\n",
      " |      hub_private_repo: Optional[bool] = None,\n",
      " |      hub_always_push: bool = False,\n",
      " |      hub_revision: Optional[str] = None,\n",
      " |      gradient_checkpointing: bool = False,\n",
      " |      gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None,\n",
      " |      include_inputs_for_metrics: bool = False,\n",
      " |      include_for_metrics: list[str] = <factory>,\n",
      " |      eval_do_concat_batches: bool = True,\n",
      " |      fp16_backend: str = 'auto',\n",
      " |      push_to_hub_model_id: Optional[str] = None,\n",
      " |      push_to_hub_organization: Optional[str] = None,\n",
      " |      push_to_hub_token: Optional[str] = None,\n",
      " |      mp_parameters: str = '',\n",
      " |      auto_find_batch_size: bool = False,\n",
      " |      full_determinism: bool = False,\n",
      " |      torchdynamo: Optional[str] = None,\n",
      " |      ray_scope: Optional[str] = 'last',\n",
      " |      ddp_timeout: int = 1800,\n",
      " |      torch_compile: bool = False,\n",
      " |      torch_compile_backend: Optional[str] = None,\n",
      " |      torch_compile_mode: Optional[str] = None,\n",
      " |      include_tokens_per_second: Optional[bool] = False,\n",
      " |      include_num_input_tokens_seen: Optional[bool] = False,\n",
      " |      neftune_noise_alpha: Optional[float] = None,\n",
      " |      optim_target_modules: Union[NoneType, str, list[str]] = None,\n",
      " |      batch_eval_metrics: bool = False,\n",
      " |      eval_on_start: bool = False,\n",
      " |      use_liger_kernel: Optional[bool] = False,\n",
      " |      liger_kernel_config: Optional[dict[str, bool]] = None,\n",
      " |      eval_use_gather_object: Optional[bool] = False,\n",
      " |      average_tokens_across_devices: Optional[bool] = True\n",
      " |  ) -> None\n",
      " |\n",
      " |  TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n",
      " |  itself**.\n",
      " |\n",
      " |  Using [`HfArgumentParser`] we can turn this class into\n",
      " |  [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n",
      " |  command line.\n",
      " |\n",
      " |  Parameters:\n",
      " |      output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n",
      " |          The output directory where the model predictions and checkpoints will be written.\n",
      " |      overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n",
      " |          If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n",
      " |          points to a checkpoint directory.\n",
      " |      do_train (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n",
      " |          by your training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      do_eval (`bool`, *optional*):\n",
      " |          Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is\n",
      " |          different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n",
      " |          training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      do_predict (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n",
      " |          intended to be used by your training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
      " |          The evaluation strategy to adopt during training. Possible values are:\n",
      " |\n",
      " |              - `\"no\"`: No evaluation is done during training.\n",
      " |              - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n",
      " |              - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      " |\n",
      " |      prediction_loss_only (`bool`, *optional*, defaults to `False`):\n",
      " |          When performing evaluation and generating predictions, only returns the loss.\n",
      " |      per_device_train_batch_size (`int`, *optional*, defaults to 8):\n",
      " |          The batch size *per device*. The **global batch size** is computed as:\n",
      " |          `per_device_train_batch_size * number_of_devices` in multi-GPU or distributed setups.\n",
      " |      per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n",
      " |          The batch size per device accelerator core/CPU for evaluation.\n",
      " |      gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
      " |          Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
      " |\n",
      " |          <Tip warning={true}>\n",
      " |\n",
      " |          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n",
      " |          evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n",
      " |\n",
      " |          </Tip>\n",
      " |\n",
      " |      eval_accumulation_steps (`int`, *optional*):\n",
      " |          Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n",
      " |          left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n",
      " |          requires more memory).\n",
      " |      eval_delay (`float`, *optional*):\n",
      " |          Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
      " |          eval_strategy.\n",
      " |      torch_empty_cache_steps (`int`, *optional*):\n",
      " |          Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.\n",
      " |\n",
      " |          <Tip>\n",
      " |\n",
      " |          This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).\n",
      " |\n",
      " |          </Tip>\n",
      " |\n",
      " |      learning_rate (`float`, *optional*, defaults to 5e-5):\n",
      " |          The initial learning rate for [`AdamW`] optimizer.\n",
      " |      weight_decay (`float`, *optional*, defaults to 0):\n",
      " |          The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n",
      " |          optimizer.\n",
      " |      adam_beta1 (`float`, *optional*, defaults to 0.9):\n",
      " |          The beta1 hyperparameter for the [`AdamW`] optimizer.\n",
      " |      adam_beta2 (`float`, *optional*, defaults to 0.999):\n",
      " |          The beta2 hyperparameter for the [`AdamW`] optimizer.\n",
      " |      adam_epsilon (`float`, *optional*, defaults to 1e-8):\n",
      " |          The epsilon hyperparameter for the [`AdamW`] optimizer.\n",
      " |      max_grad_norm (`float`, *optional*, defaults to 1.0):\n",
      " |          Maximum gradient norm (for gradient clipping).\n",
      " |      num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
      " |          Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n",
      " |          the last epoch before stopping training).\n",
      " |      max_steps (`int`, *optional*, defaults to -1):\n",
      " |          If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
      " |          For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
      " |          `max_steps` is reached.\n",
      " |      lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
      " |          The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
      " |      lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n",
      " |          The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n",
      " |      warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
      " |          Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
      " |      warmup_steps (`int`, *optional*, defaults to 0):\n",
      " |          Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n",
      " |      log_level (`str`, *optional*, defaults to `passive`):\n",
      " |          Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n",
      " |          'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n",
      " |          current log level for the Transformers library (which will be `\"warning\"` by default).\n",
      " |      log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n",
      " |          Logger log level to use on replicas. Same choices as `log_level`\"\n",
      " |      log_on_each_node (`bool`, *optional*, defaults to `True`):\n",
      " |          In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
      " |          node.\n",
      " |      logging_dir (`str`, *optional*):\n",
      " |          [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n",
      " |          *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n",
      " |      logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      " |          The logging strategy to adopt during training. Possible values are:\n",
      " |\n",
      " |              - `\"no\"`: No logging is done during training.\n",
      " |              - `\"epoch\"`: Logging is done at the end of each epoch.\n",
      " |              - `\"steps\"`: Logging is done every `logging_steps`.\n",
      " |\n",
      " |      logging_first_step (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to log the first `global_step` or not.\n",
      " |      logging_steps (`int` or `float`, *optional*, defaults to 500):\n",
      " |          Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n",
      " |          range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
      " |      logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n",
      " |          or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
      " |\n",
      " |          <Tip>\n",
      " |\n",
      " |          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
      " |          gradient is computed or applied to the model.\n",
      " |\n",
      " |          </Tip>\n",
      " |\n",
      " |      save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `\"steps\"`):\n",
      " |          The checkpoint save strategy to adopt during training. Possible values are:\n",
      " |\n",
      " |              - `\"no\"`: No save is done during training.\n",
      " |              - `\"epoch\"`: Save is done at the end of each epoch.\n",
      " |              - `\"steps\"`: Save is done every `save_steps`.\n",
      " |              - `\"best\"`: Save is done whenever a new `best_metric` is achieved.\n",
      " |\n",
      " |              If `\"epoch\"` or `\"steps\"` is chosen, saving will also be performed at the\n",
      " |              very end of training, always.\n",
      " |      save_steps (`int` or `float`, *optional*, defaults to 500):\n",
      " |          Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n",
      " |          float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n",
      " |      save_total_limit (`int`, *optional*):\n",
      " |          If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      " |          `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n",
      " |          `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n",
      " |          `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n",
      " |          alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n",
      " |          checkpoints are saved: the last one and the best one (if they are different).\n",
      " |      save_safetensors (`bool`, *optional*, defaults to `True`):\n",
      " |          Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n",
      " |          default `torch.load` and `torch.save`.\n",
      " |      save_on_each_node (`bool`, *optional*, defaults to `False`):\n",
      " |          When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n",
      " |          the main one.\n",
      " |\n",
      " |          This should not be activated when the different nodes use the same storage as the files will be saved with\n",
      " |          the same names for each node.\n",
      " |      save_only_model (`bool`, *optional*, defaults to `False`):\n",
      " |          When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n",
      " |          Note that when this is true, you won't be able to resume training from checkpoint.\n",
      " |          This enables you to save storage by not storing the optimizer, scheduler & rng state.\n",
      " |          You can only load the model using `from_pretrained` with this option set to `True`.\n",
      " |      restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to restore the callback states from the checkpoint. If `True`, will override\n",
      " |          callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n",
      " |      use_cpu (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n",
      " |      seed (`int`, *optional*, defaults to 42):\n",
      " |          Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n",
      " |          [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n",
      " |      data_seed (`int`, *optional*):\n",
      " |          Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
      " |          same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n",
      " |          seed.\n",
      " |      jit_mode_eval (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to use PyTorch jit trace for inference.\n",
      " |      use_ipex (`bool`, *optional*, defaults to `False`):\n",
      " |          Use Intel extension for PyTorch when it is available. [IPEX\n",
      " |          installation](https://github.com/intel/intel-extension-for-pytorch).\n",
      " |      bf16 (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n",
      " |          NVIDIA architecture or Intel XPU or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n",
      " |      fp16 (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n",
      " |      fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n",
      " |          For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n",
      " |          the [Apex documentation](https://nvidia.github.io/apex/amp).\n",
      " |      fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      " |          This argument is deprecated. Use `half_precision_backend` instead.\n",
      " |      half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n",
      " |          The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n",
      " |          use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n",
      " |          requested backend.\n",
      " |      bf16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      " |          metric values. This is an experimental API and it may change.\n",
      " |      fp16_full_eval (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n",
      " |          metric values.\n",
      " |      tf32 (`bool`, *optional*):\n",
      " |          Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n",
      " |          on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n",
      " |          the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n",
      " |          experimental API and it may change.\n",
      " |      local_rank (`int`, *optional*, defaults to -1):\n",
      " |          Rank of the process during distributed training.\n",
      " |      ddp_backend (`str`, *optional*):\n",
      " |          The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n",
      " |      tpu_num_cores (`int`, *optional*):\n",
      " |          When training on TPU, the number of TPU cores (automatically passed by launcher script).\n",
      " |      dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n",
      " |          or not.\n",
      " |      eval_steps (`int` or `float`, *optional*):\n",
      " |          Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n",
      " |          value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n",
      " |          will be interpreted as ratio of total training steps.\n",
      " |      dataloader_num_workers (`int`, *optional*, defaults to 0):\n",
      " |          Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n",
      " |          main process.\n",
      " |      past_index (`int`, *optional*, defaults to -1):\n",
      " |          Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n",
      " |          the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n",
      " |          use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n",
      " |          training step under the keyword argument `mems`.\n",
      " |      run_name (`str`, *optional*, defaults to `output_dir`):\n",
      " |          A descriptor for the run. Typically used for [trackio](https://github.com/gradio-app/trackio),\n",
      " |          [wandb](https://www.wandb.com/), [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and\n",
      " |          [swanlab](https://swanlab.cn) logging. If not specified, will be the same as `output_dir`.\n",
      " |      disable_tqdm (`bool`, *optional*):\n",
      " |          Whether or not to disable the tqdm progress bars and table of metrics produced by\n",
      " |          [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n",
      " |          set to warn or lower (default), `False` otherwise.\n",
      " |      remove_unused_columns (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not to automatically remove the columns unused by the model forward method.\n",
      " |      label_names (`list[str]`, *optional*):\n",
      " |          The list of keys in your dictionary of inputs that correspond to the labels.\n",
      " |\n",
      " |          Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n",
      " |          except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n",
      " |          `[\"start_positions\", \"end_positions\"]` keys.\n",
      " |\n",
      " |          You should only specify `label_names` if you're using custom label names or if your model's `forward` consumes multiple label tensors (e.g., extractive QA).\n",
      " |      load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to load the best model found during training at the end of training. When this option is\n",
      " |          enabled, the best checkpoint will always be saved. See\n",
      " |          [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n",
      " |          for more.\n",
      " |\n",
      " |          <Tip>\n",
      " |\n",
      " |          When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n",
      " |          the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n",
      " |\n",
      " |          </Tip>\n",
      " |\n",
      " |      metric_for_best_model (`str`, *optional*):\n",
      " |          Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n",
      " |          models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`.\n",
      " |\n",
      " |          If not specified, this will default to `\"loss\"` when either `load_best_model_at_end == True`\n",
      " |          or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).\n",
      " |\n",
      " |          If you set this value, `greater_is_better` will default to `True` unless the name ends with \"loss\".\n",
      " |          Don't forget to set it to `False` if your metric is better when lower.\n",
      " |      greater_is_better (`bool`, *optional*):\n",
      " |          Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n",
      " |          should have a greater metric or not. Will default to:\n",
      " |\n",
      " |          - `True` if `metric_for_best_model` is set to a value that doesn't end in `\"loss\"`.\n",
      " |          - `False` if `metric_for_best_model` is not set, or set to a value that ends in `\"loss\"`.\n",
      " |      ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
      " |          When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n",
      " |          stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n",
      " |          can take a long time) but will not yield the same results as the interrupted training would have.\n",
      " |      fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):\n",
      " |          Use PyTorch Distributed Parallel Training (in distributed training only).\n",
      " |\n",
      " |          A list of options along the following:\n",
      " |\n",
      " |          - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n",
      " |          - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n",
      " |          - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n",
      " |          - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n",
      " |          - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n",
      " |            `\"shard_grad_op\"`).\n",
      " |          - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n",
      " |      fsdp_config (`str` or `dict`, *optional*):\n",
      " |          Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n",
      " |          fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n",
      " |\n",
      " |          A List of config and its options:\n",
      " |              - min_num_params (`int`, *optional*, defaults to `0`):\n",
      " |                  FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n",
      " |                  passed).\n",
      " |              - transformer_layer_cls_to_wrap (`list[str]`, *optional*):\n",
      " |                  List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n",
      " |                  `T5Block` .... (useful only when `fsdp` flag is passed).\n",
      " |              - backward_prefetch (`str`, *optional*)\n",
      " |                  FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n",
      " |                  `fsdp` field is passed).\n",
      " |\n",
      " |                  A list of options along the following:\n",
      " |\n",
      " |                  - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n",
      " |                    gradient\n",
      " |                      computation.\n",
      " |                  - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n",
      " |                    parameter’s\n",
      " |                      gradient computation.\n",
      " |              - forward_prefetch (`bool`, *optional*, defaults to `False`)\n",
      " |                  FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n",
      " |                   If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n",
      " |                   forward pass.\n",
      " |              - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n",
      " |                  FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n",
      " |                   If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n",
      " |                   all-gathers.\n",
      " |              - use_orig_params (`bool`, *optional*, defaults to `True`)\n",
      " |                  If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n",
      " |                  frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n",
      " |                  refer this\n",
      " |                  [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n",
      " |              - sync_module_states (`bool`, *optional*, defaults to `True`)\n",
      " |                  If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n",
      " |                  ensure they are the same across all ranks after initialization\n",
      " |              - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)\n",
      " |                  If `\"True\"`, only the first process loads the pretrained model checkpoint while all other processes\n",
      " |                  have empty weights.  When this setting as `\"True\"`, `sync_module_states` also must to be `\"True\"`,\n",
      " |                  otherwise all the processes except the main process would have random weights leading to unexpected\n",
      " |                  behaviour during training.\n",
      " |              - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      " |                  If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n",
      " |                  certain layers and recomputing them during a backward pass. Effectively, this trades extra\n",
      " |                  computation time for reduced memory usage.\n",
      " |              - xla (`bool`, *optional*, defaults to `False`):\n",
      " |                  Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n",
      " |                  and its API may evolve in the future.\n",
      " |              - xla_fsdp_settings (`dict`, *optional*)\n",
      " |                  The value is a dictionary which stores the XLA FSDP wrapping parameters.\n",
      " |\n",
      " |                  For a complete list of options, please see [here](\n",
      " |                  https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n",
      " |              - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n",
      " |                  Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n",
      " |                  used when the xla flag is set to true, and an auto wrapping policy is specified through\n",
      " |                  fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n",
      " |      deepspeed (`str` or `dict`, *optional*):\n",
      " |          Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n",
      " |          evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n",
      " |          `ds_config.json`) or an already loaded json file as a `dict`\"\n",
      " |\n",
      " |          <Tip warning={true}>\n",
      " |              If enabling any Zero-init, make sure that your model is not initialized until\n",
      " |              *after* initializing the `TrainingArguments`, else it will not be applied.\n",
      " |          </Tip>\n",
      " |\n",
      " |      accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):\n",
      " |          Config to be used with the internal `Accelerator` implementation. The value is either a location of\n",
      " |          accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,\n",
      " |          or an instance of [`~trainer_pt_utils.AcceleratorConfig`].\n",
      " |\n",
      " |          A list of config and its options:\n",
      " |              - split_batches (`bool`, *optional*, defaults to `False`):\n",
      " |                  Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n",
      " |                  `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n",
      " |                  round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n",
      " |                  in your script multiplied by the number of processes.\n",
      " |              - dispatch_batches (`bool`, *optional*):\n",
      " |                  If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n",
      " |                  and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n",
      " |                  underlying dataset is an `IterableDataset`, `False` otherwise.\n",
      " |              - even_batches (`bool`, *optional*, defaults to `True`):\n",
      " |                  If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n",
      " |                  dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n",
      " |                  all workers.\n",
      " |              - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n",
      " |                  Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n",
      " |                  training results are fully reproducible using a different sampling technique. While seed-to-seed results\n",
      " |                  may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n",
      " |                  also be ran with [`~utils.set_seed`] for the best results.\n",
      " |              - use_configured_state (`bool`, *optional*, defaults to `False`):\n",
      " |                  Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n",
      " |                  If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n",
      " |                  with hyperparameter tuning.\n",
      " |      parallelism_config (`ParallelismConfig`, *optional*):\n",
      " |          Parallelism configuration for the training run. Requires Accelerate `1.10.1`\n",
      " |      label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n",
      " |          The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n",
      " |          labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n",
      " |          label_smoothing_factor/num_labels` respectively.\n",
      " |      debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n",
      " |          Enable one or more debug features. This is an experimental feature.\n",
      " |\n",
      " |          Possible options are:\n",
      " |\n",
      " |          - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n",
      " |            the event\n",
      " |          - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n",
      " |\n",
      " |          The options should be separated by whitespaces.\n",
      " |      optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"` (for torch>=2.8 `\"adamw_torch_fused\"`)):\n",
      " |          The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n",
      " |          \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n",
      " |          for a full list of optimizers.\n",
      " |      optim_args (`str`, *optional*):\n",
      " |          Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n",
      " |      group_by_length (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n",
      " |          padding applied and be more efficient). Only useful if applying dynamic padding.\n",
      " |      length_column_name (`str`, *optional*, defaults to `\"length\"`):\n",
      " |          Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n",
      " |          than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n",
      " |          instance of `Dataset`.\n",
      " |      report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n",
      " |          The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
      " |          `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n",
      " |          `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all integrations\n",
      " |          installed, `\"none\"` for no integrations.\n",
      " |      ddp_find_unused_parameters (`bool`, *optional*):\n",
      " |          When using distributed training, the value of the flag `find_unused_parameters` passed to\n",
      " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      " |      ddp_bucket_cap_mb (`int`, *optional*):\n",
      " |          When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n",
      " |      ddp_broadcast_buffers (`bool`, *optional*):\n",
      " |          When using distributed training, the value of the flag `broadcast_buffers` passed to\n",
      " |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n",
      " |      dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
      " |      dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n",
      " |          If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n",
      " |          This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n",
      " |          increase RAM usage. Will default to `False`.\n",
      " |      dataloader_prefetch_factor (`int`, *optional*):\n",
      " |          Number of batches loaded in advance by each worker.\n",
      " |          2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
      " |      skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n",
      " |          down the training and evaluation speed.\n",
      " |      push_to_hub (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n",
      " |          `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n",
      " |          will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n",
      " |          [`~Trainer.save_model`] will also trigger a push.\n",
      " |\n",
      " |          <Tip warning={true}>\n",
      " |\n",
      " |          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n",
      " |          pushed.\n",
      " |\n",
      " |          </Tip>\n",
      " |\n",
      " |      resume_from_checkpoint (`str`, *optional*):\n",
      " |          The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n",
      " |          [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n",
      " |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n",
      " |      hub_model_id (`str`, *optional*):\n",
      " |          The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
      " |          which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n",
      " |          for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n",
      " |          `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n",
      " |          name of `output_dir`.\n",
      " |\n",
      " |          Will default to the name of `output_dir`.\n",
      " |      hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
      " |          Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
      " |\n",
      " |          - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
      " |            draft of a model card when the [`~Trainer.save_model`] method is called.\n",
      " |          - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n",
      " |            a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
      " |            training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
      " |            finished. A last push is made with the final model at the end of training.\n",
      " |          - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
      " |            last-checkpoint, allowing you to resume training easily with\n",
      " |            `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
      " |          - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n",
      " |            folder (so you will get one checkpoint folder per folder in your final repository)\n",
      " |\n",
      " |      hub_token (`str`, *optional*):\n",
      " |          The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n",
      " |          `hf auth login`.\n",
      " |      hub_private_repo (`bool`, *optional*):\n",
      " |          Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |      hub_always_push (`bool`, *optional*, defaults to `False`):\n",
      " |          Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n",
      " |      hub_revision (`str`, *optional*):\n",
      " |          The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n",
      " |      gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      " |          If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
      " |      gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n",
      " |          Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n",
      " |      include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n",
      " |          This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n",
      " |      include_for_metrics (`list[str]`, *optional*, defaults to `[]`):\n",
      " |          Include additional data in the `compute_metrics` function if needed for metrics computation.\n",
      " |          Possible options to add to `include_for_metrics` list:\n",
      " |          - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n",
      " |          - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n",
      " |      eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n",
      " |          will instead store them as lists, with each batch kept separate.\n",
      " |      auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
      " |          Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n",
      " |          CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
      " |      full_determinism (`bool`, *optional*, defaults to `False`)\n",
      " |          If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n",
      " |          distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n",
      " |      torchdynamo (`str`, *optional*):\n",
      " |          If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n",
      " |          `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n",
      " |      ray_scope (`str`, *optional*, defaults to `\"last\"`):\n",
      " |          The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n",
      " |          then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n",
      " |          are also available. See the [Ray documentation](\n",
      " |          https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n",
      " |          more options.\n",
      " |      ddp_timeout (`int`, *optional*, defaults to 1800):\n",
      " |          The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n",
      " |          performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n",
      " |          (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n",
      " |          information.\n",
      " |      use_mps_device (`bool`, *optional*, defaults to `False`):\n",
      " |          This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n",
      " |      torch_compile (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether or not to compile the model using PyTorch 2.0\n",
      " |          [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n",
      " |\n",
      " |          This will use the best defaults for the [`torch.compile`\n",
      " |          API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n",
      " |          You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n",
      " |          don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n",
      " |\n",
      " |          This flag and the whole compile API is experimental and subject to change in future releases.\n",
      " |      torch_compile_backend (`str`, *optional*):\n",
      " |          The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
      " |\n",
      " |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
      " |\n",
      " |          This flag is experimental and subject to change in future releases.\n",
      " |      torch_compile_mode (`str`, *optional*):\n",
      " |          The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n",
      " |\n",
      " |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n",
      " |\n",
      " |          This flag is experimental and subject to change in future releases.\n",
      " |      include_tokens_per_second (`bool`, *optional*):\n",
      " |          Whether or not to compute the number of tokens per second per device for training speed metrics.\n",
      " |\n",
      " |          This will iterate over the entire training dataloader once beforehand,\n",
      " |\n",
      " |          and will slow down the entire process.\n",
      " |\n",
      " |      include_num_input_tokens_seen (`bool`, *optional*):\n",
      " |          Whether or not to track the number of input tokens seen throughout training.\n",
      " |\n",
      " |          May be slower in distributed training as gather operations must be called.\n",
      " |\n",
      " |      neftune_noise_alpha (`Optional[float]`):\n",
      " |          If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n",
      " |          for instruction fine-tuning. Check out the [original paper](https://huggingface.co/papers/2310.05914) and the\n",
      " |          [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n",
      " |          `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n",
      " |      optim_target_modules (`Union[str, list[str]]`, *optional*):\n",
      " |          The target modules to optimize, i.e. the module names that you would like to train.\n",
      " |          Currently used for the GaLore algorithm (https://huggingface.co/papers/2403.03507) and APOLLO algorithm (https://huggingface.co/papers/2412.05270).\n",
      " |          See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n",
      " |          You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n",
      " |\n",
      " |      batch_eval_metrics (`Optional[bool]`, defaults to `False`):\n",
      " |          If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics\n",
      " |          rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function\n",
      " |          that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global\n",
      " |          summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.\n",
      " |\n",
      " |      eval_on_start (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n",
      " |\n",
      " |      eval_use_gather_object (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n",
      " |\n",
      " |      use_liger_kernel (`bool`, *optional*, defaults to `False`):\n",
      " |          Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.\n",
      " |          It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n",
      " |          flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n",
      " |\n",
      " |      liger_kernel_config (`Optional[dict]`, *optional*):\n",
      " |          Configuration to be used for Liger Kernel. When use_liger_kernel=True, this dict is passed as keyword arguments to the\n",
      " |          `_apply_liger_kernel_to_instance` function, which specifies which kernels to apply. Available options vary by model but typically\n",
      " |          include: 'rope', 'swiglu', 'cross_entropy', 'fused_linear_cross_entropy', 'rms_norm', etc. If `None`, use the default kernel configurations.\n",
      " |\n",
      " |      average_tokens_across_devices (`bool`, *optional*, defaults to `True`):\n",
      " |          Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n",
      " |          num_tokens_in_batch for precise loss calculation. Reference:\n",
      " |          https://github.com/huggingface/transformers/issues/34242\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __init__(\n",
      " |      self,\n",
      " |      output_dir: Optional[str] = None,\n",
      " |      overwrite_output_dir: bool = False,\n",
      " |      do_train: bool = False,\n",
      " |      do_eval: bool = False,\n",
      " |      do_predict: bool = False,\n",
      " |      eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no',\n",
      " |      prediction_loss_only: bool = False,\n",
      " |      per_device_train_batch_size: int = 8,\n",
      " |      per_device_eval_batch_size: int = 8,\n",
      " |      per_gpu_train_batch_size: Optional[int] = None,\n",
      " |      per_gpu_eval_batch_size: Optional[int] = None,\n",
      " |      gradient_accumulation_steps: int = 1,\n",
      " |      eval_accumulation_steps: Optional[int] = None,\n",
      " |      eval_delay: Optional[float] = 0,\n",
      " |      torch_empty_cache_steps: Optional[int] = None,\n",
      " |      learning_rate: float = 5e-05,\n",
      " |      weight_decay: float = 0.0,\n",
      " |      adam_beta1: float = 0.9,\n",
      " |      adam_beta2: float = 0.999,\n",
      " |      adam_epsilon: float = 1e-08,\n",
      " |      max_grad_norm: float = 1.0,\n",
      " |      num_train_epochs: float = 3.0,\n",
      " |      max_steps: int = -1,\n",
      " |      lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear',\n",
      " |      lr_scheduler_kwargs: Union[dict[str, Any], str, NoneType] = <factory>,\n",
      " |      warmup_ratio: float = 0.0,\n",
      " |      warmup_steps: int = 0,\n",
      " |      log_level: str = 'passive',\n",
      " |      log_level_replica: str = 'warning',\n",
      " |      log_on_each_node: bool = True,\n",
      " |      logging_dir: Optional[str] = None,\n",
      " |      logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps',\n",
      " |      logging_first_step: bool = False,\n",
      " |      logging_steps: float = 500,\n",
      " |      logging_nan_inf_filter: bool = True,\n",
      " |      save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps',\n",
      " |      save_steps: float = 500,\n",
      " |      save_total_limit: Optional[int] = None,\n",
      " |      save_safetensors: Optional[bool] = True,\n",
      " |      save_on_each_node: bool = False,\n",
      " |      save_only_model: bool = False,\n",
      " |      restore_callback_states_from_checkpoint: bool = False,\n",
      " |      no_cuda: bool = False,\n",
      " |      use_cpu: bool = False,\n",
      " |      use_mps_device: bool = False,\n",
      " |      seed: int = 42,\n",
      " |      data_seed: Optional[int] = None,\n",
      " |      jit_mode_eval: bool = False,\n",
      " |      use_ipex: bool = False,\n",
      " |      bf16: bool = False,\n",
      " |      fp16: bool = False,\n",
      " |      fp16_opt_level: str = 'O1',\n",
      " |      half_precision_backend: str = 'auto',\n",
      " |      bf16_full_eval: bool = False,\n",
      " |      fp16_full_eval: bool = False,\n",
      " |      tf32: Optional[bool] = None,\n",
      " |      local_rank: int = -1,\n",
      " |      ddp_backend: Optional[str] = None,\n",
      " |      tpu_num_cores: Optional[int] = None,\n",
      " |      tpu_metrics_debug: bool = False,\n",
      " |      debug: Union[str, list[transformers.debug_utils.DebugOption]] = '',\n",
      " |      dataloader_drop_last: bool = False,\n",
      " |      eval_steps: Optional[float] = None,\n",
      " |      dataloader_num_workers: int = 0,\n",
      " |      dataloader_prefetch_factor: Optional[int] = None,\n",
      " |      past_index: int = -1,\n",
      " |      run_name: Optional[str] = None,\n",
      " |      disable_tqdm: Optional[bool] = None,\n",
      " |      remove_unused_columns: Optional[bool] = True,\n",
      " |      label_names: Optional[list[str]] = None,\n",
      " |      load_best_model_at_end: Optional[bool] = False,\n",
      " |      metric_for_best_model: Optional[str] = None,\n",
      " |      greater_is_better: Optional[bool] = None,\n",
      " |      ignore_data_skip: bool = False,\n",
      " |      fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '',\n",
      " |      fsdp_min_num_params: int = 0,\n",
      " |      fsdp_config: Union[dict[str, Any], str, NoneType] = None,\n",
      " |      fsdp_transformer_layer_cls_to_wrap: Optional[str] = None,\n",
      " |      accelerator_config: Union[dict, str, NoneType] = None,\n",
      " |      parallelism_config: Optional[ForwardRef('ParallelismConfig')] = None,\n",
      " |      deepspeed: Union[dict, str, NoneType] = None,\n",
      " |      label_smoothing_factor: float = 0.0,\n",
      " |      optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch_fused',\n",
      " |      optim_args: Optional[str] = None,\n",
      " |      adafactor: bool = False,\n",
      " |      group_by_length: bool = False,\n",
      " |      length_column_name: Optional[str] = 'length',\n",
      " |      report_to: Union[NoneType, str, list[str]] = None,\n",
      " |      ddp_find_unused_parameters: Optional[bool] = None,\n",
      " |      ddp_bucket_cap_mb: Optional[int] = None,\n",
      " |      ddp_broadcast_buffers: Optional[bool] = None,\n",
      " |      dataloader_pin_memory: bool = True,\n",
      " |      dataloader_persistent_workers: bool = False,\n",
      " |      skip_memory_metrics: bool = True,\n",
      " |      use_legacy_prediction_loop: bool = False,\n",
      " |      push_to_hub: bool = False,\n",
      " |      resume_from_checkpoint: Optional[str] = None,\n",
      " |      hub_model_id: Optional[str] = None,\n",
      " |      hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save',\n",
      " |      hub_token: Optional[str] = None,\n",
      " |      hub_private_repo: Optional[bool] = None,\n",
      " |      hub_always_push: bool = False,\n",
      " |      hub_revision: Optional[str] = None,\n",
      " |      gradient_checkpointing: bool = False,\n",
      " |      gradient_checkpointing_kwargs: Union[dict[str, Any], str, NoneType] = None,\n",
      " |      include_inputs_for_metrics: bool = False,\n",
      " |      include_for_metrics: list[str] = <factory>,\n",
      " |      eval_do_concat_batches: bool = True,\n",
      " |      fp16_backend: str = 'auto',\n",
      " |      push_to_hub_model_id: Optional[str] = None,\n",
      " |      push_to_hub_organization: Optional[str] = None,\n",
      " |      push_to_hub_token: Optional[str] = None,\n",
      " |      mp_parameters: str = '',\n",
      " |      auto_find_batch_size: bool = False,\n",
      " |      full_determinism: bool = False,\n",
      " |      torchdynamo: Optional[str] = None,\n",
      " |      ray_scope: Optional[str] = 'last',\n",
      " |      ddp_timeout: int = 1800,\n",
      " |      torch_compile: bool = False,\n",
      " |      torch_compile_backend: Optional[str] = None,\n",
      " |      torch_compile_mode: Optional[str] = None,\n",
      " |      include_tokens_per_second: Optional[bool] = False,\n",
      " |      include_num_input_tokens_seen: Optional[bool] = False,\n",
      " |      neftune_noise_alpha: Optional[float] = None,\n",
      " |      optim_target_modules: Union[NoneType, str, list[str]] = None,\n",
      " |      batch_eval_metrics: bool = False,\n",
      " |      eval_on_start: bool = False,\n",
      " |      use_liger_kernel: Optional[bool] = False,\n",
      " |      liger_kernel_config: Optional[dict[str, bool]] = None,\n",
      " |      eval_use_gather_object: Optional[bool] = False,\n",
      " |      average_tokens_across_devices: Optional[bool] = True\n",
      " |  ) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  __post_init__(self)\n",
      " |\n",
      " |  __replace__ = _replace(self, /, **changes) from dataclasses\n",
      " |\n",
      " |  __repr__ = __str__(self)\n",
      " |\n",
      " |  __str__(self)\n",
      " |      Return str(self).\n",
      " |\n",
      " |  get_process_log_level(self)\n",
      " |      Returns the log level to be used depending on whether this process is the main process of node 0, main process\n",
      " |      of node non-0, or a non-main process.\n",
      " |\n",
      " |      For the main process the log level defaults to the logging level set (`logging.WARNING` if you didn't do\n",
      " |      anything) unless overridden by `log_level` argument.\n",
      " |\n",
      " |      For the replica processes the log level defaults to `logging.WARNING` unless overridden by `log_level_replica`\n",
      " |      argument.\n",
      " |\n",
      " |      The choice between the main and replica process settings is made according to the return value of `should_log`.\n",
      " |\n",
      " |  get_warmup_steps(self, num_training_steps: int)\n",
      " |      Get number of steps used for a linear warmup.\n",
      " |\n",
      " |  main_process_first(self, local=True, desc='work')\n",
      " |      A context manager for torch distributed environment where on needs to do something on the main process, while\n",
      " |      blocking replicas, and when it's finished releasing the replicas.\n",
      " |\n",
      " |      One such use is for `datasets`'s `map` feature which to be efficient should be run once on the main process,\n",
      " |      which upon completion saves a cached version of results and which then automatically gets loaded by the\n",
      " |      replicas.\n",
      " |\n",
      " |      Args:\n",
      " |          local (`bool`, *optional*, defaults to `True`):\n",
      " |              if `True` first means process of rank 0 of each node if `False` first means process of rank 0 of node\n",
      " |              rank 0 In multi-node environment with a shared filesystem you most likely will want to use\n",
      " |              `local=False` so that only the main process of the first node will do the processing. If however, the\n",
      " |              filesystem is not shared, then the main process of each node will need to do the processing, which is\n",
      " |              the default behavior.\n",
      " |          desc (`str`, *optional*, defaults to `\"work\"`):\n",
      " |              a work description to be used in debug logs\n",
      " |\n",
      " |  set_dataloader(\n",
      " |      self,\n",
      " |      train_batch_size: int = 8,\n",
      " |      eval_batch_size: int = 8,\n",
      " |      drop_last: bool = False,\n",
      " |      num_workers: int = 0,\n",
      " |      pin_memory: bool = True,\n",
      " |      persistent_workers: bool = False,\n",
      " |      prefetch_factor: Optional[int] = None,\n",
      " |      auto_find_batch_size: bool = False,\n",
      " |      ignore_data_skip: bool = False,\n",
      " |      sampler_seed: Optional[int] = None\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to the dataloaders creation.\n",
      " |\n",
      " |      Args:\n",
      " |          drop_last (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch\n",
      " |              size) or not.\n",
      " |          num_workers (`int`, *optional*, defaults to 0):\n",
      " |              Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in\n",
      " |              the main process.\n",
      " |          pin_memory (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether you want to pin memory in data loaders or not. Will default to `True`.\n",
      " |          persistent_workers (`bool`, *optional*, defaults to `False`):\n",
      " |              If True, the data loader will not shut down the worker processes after a dataset has been consumed\n",
      " |              once. This allows to maintain the workers Dataset instances alive. Can potentially speed up training,\n",
      " |              but will increase RAM usage. Will default to `False`.\n",
      " |          prefetch_factor (`int`, *optional*):\n",
      " |              Number of batches loaded in advance by each worker.\n",
      " |              2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n",
      " |          auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n",
      " |              Whether to find a batch size that will fit into memory automatically through exponential decay,\n",
      " |              avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n",
      " |          ignore_data_skip (`bool`, *optional*, defaults to `False`):\n",
      " |              When resuming training, whether or not to skip the epochs and batches to get the data loading at the\n",
      " |              same stage as in the previous training. If set to `True`, the training will begin faster (as that\n",
      " |              skipping step can take a long time) but will not yield the same results as the interrupted training\n",
      " |              would have.\n",
      " |          sampler_seed (`int`, *optional*):\n",
      " |              Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n",
      " |              same seed as `self.seed`. This can be used to ensure reproducibility of data sampling, independent of\n",
      " |              the model seed.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)\n",
      " |      >>> args.per_device_train_batch_size\n",
      " |      16\n",
      " |      ```\n",
      " |\n",
      " |  set_evaluate(\n",
      " |      self,\n",
      " |      strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'no',\n",
      " |      steps: int = 500,\n",
      " |      batch_size: int = 8,\n",
      " |      accumulation_steps: Optional[int] = None,\n",
      " |      delay: Optional[float] = None,\n",
      " |      loss_only: bool = False,\n",
      " |      jit_mode: bool = False\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to evaluation.\n",
      " |\n",
      " |      Args:\n",
      " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n",
      " |              The evaluation strategy to adopt during training. Possible values are:\n",
      " |\n",
      " |                  - `\"no\"`: No evaluation is done during training.\n",
      " |                  - `\"steps\"`: Evaluation is done (and logged) every `steps`.\n",
      " |                  - `\"epoch\"`: Evaluation is done at the end of each epoch.\n",
      " |\n",
      " |              Setting a `strategy` different from `\"no\"` will set `self.do_eval` to `True`.\n",
      " |          steps (`int`, *optional*, defaults to 500):\n",
      " |              Number of update steps between two evaluations if `strategy=\"steps\"`.\n",
      " |          batch_size (`int` *optional*, defaults to 8):\n",
      " |              The batch size per device (GPU/TPU core/CPU...) used for evaluation.\n",
      " |          accumulation_steps (`int`, *optional*):\n",
      " |              Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\n",
      " |              If left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster\n",
      " |              but requires more memory).\n",
      " |          delay (`float`, *optional*):\n",
      " |              Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n",
      " |              eval_strategy.\n",
      " |          loss_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Ignores all outputs except the loss.\n",
      " |          jit_mode (`bool`, *optional*):\n",
      " |              Whether or not to use PyTorch jit trace for inference.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_evaluate(strategy=\"steps\", steps=100)\n",
      " |      >>> args.eval_steps\n",
      " |      100\n",
      " |      ```\n",
      " |\n",
      " |  set_logging(\n",
      " |      self,\n",
      " |      strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps',\n",
      " |      steps: int = 500,\n",
      " |      report_to: Union[str, list[str]] = 'none',\n",
      " |      level: str = 'passive',\n",
      " |      first_step: bool = False,\n",
      " |      nan_inf_filter: bool = False,\n",
      " |      on_each_node: bool = False,\n",
      " |      replica_level: str = 'passive'\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to logging.\n",
      " |\n",
      " |      Args:\n",
      " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      " |              The logging strategy to adopt during training. Possible values are:\n",
      " |\n",
      " |                  - `\"no\"`: No logging is done during training.\n",
      " |                  - `\"epoch\"`: Logging is done at the end of each epoch.\n",
      " |                  - `\"steps\"`: Logging is done every `logging_steps`.\n",
      " |\n",
      " |          steps (`int`, *optional*, defaults to 500):\n",
      " |              Number of update steps between two logs if `strategy=\"steps\"`.\n",
      " |          level (`str`, *optional*, defaults to `\"passive\"`):\n",
      " |              Logger log level to use on the main process. Possible choices are the log levels as strings: `\"debug\"`,\n",
      " |              `\"info\"`, `\"warning\"`, `\"error\"` and `\"critical\"`, plus a `\"passive\"` level which doesn't set anything\n",
      " |              and lets the application set the level.\n",
      " |          report_to (`str` or `list[str]`, *optional*, defaults to `\"all\"`):\n",
      " |              The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n",
      " |              `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`,\n",
      " |              `\"neptune\"`, `\"swanlab\"`, `\"tensorboard\"`, `\"trackio\"` and `\"wandb\"`. Use `\"all\"` to report to all\n",
      " |              integrations installed, `\"none\"` for no integrations.\n",
      " |          first_step (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to log and evaluate the first `global_step` or not.\n",
      " |          nan_inf_filter (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is\n",
      " |              `nan` or `inf` is filtered and the average loss of the current logging window is taken instead.\n",
      " |\n",
      " |              <Tip>\n",
      " |\n",
      " |              `nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n",
      " |              gradient is computed or applied to the model.\n",
      " |\n",
      " |              </Tip>\n",
      " |\n",
      " |          on_each_node (`bool`, *optional*, defaults to `True`):\n",
      " |              In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n",
      " |              node.\n",
      " |          replica_level (`str`, *optional*, defaults to `\"passive\"`):\n",
      " |              Logger log level to use on replicas. Same choices as `log_level`\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_logging(strategy=\"steps\", steps=100)\n",
      " |      >>> args.logging_steps\n",
      " |      100\n",
      " |      ```\n",
      " |\n",
      " |  set_lr_scheduler(\n",
      " |      self,\n",
      " |      name: Union[str, transformers.trainer_utils.SchedulerType] = 'linear',\n",
      " |      num_epochs: float = 3.0,\n",
      " |      max_steps: int = -1,\n",
      " |      warmup_ratio: float = 0,\n",
      " |      warmup_steps: int = 0\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n",
      " |\n",
      " |      Args:\n",
      " |          name (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n",
      " |              The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n",
      " |          num_epochs(`float`, *optional*, defaults to 3.0):\n",
      " |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n",
      " |              of the last epoch before stopping training).\n",
      " |          max_steps (`int`, *optional*, defaults to -1):\n",
      " |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
      " |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
      " |              `max_steps` is reached.\n",
      " |          warmup_ratio (`float`, *optional*, defaults to 0.0):\n",
      " |              Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n",
      " |          warmup_steps (`int`, *optional*, defaults to 0):\n",
      " |              Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of\n",
      " |              `warmup_ratio`.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n",
      " |      >>> args.warmup_ratio\n",
      " |      0.05\n",
      " |      ```\n",
      " |\n",
      " |  set_optimizer(\n",
      " |      self,\n",
      " |      name: Union[str, transformers.training_args.OptimizerNames] = 'adamw_torch',\n",
      " |      learning_rate: float = 5e-05,\n",
      " |      weight_decay: float = 0,\n",
      " |      beta1: float = 0.9,\n",
      " |      beta2: float = 0.999,\n",
      " |      epsilon: float = 1e-08,\n",
      " |      args: Optional[str] = None\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to the optimizer and its hyperparameters.\n",
      " |\n",
      " |      Args:\n",
      " |          name (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n",
      " |              The optimizer to use: `\"adamw_torch\"`, `\"adamw_torch_fused\"`, `\"adamw_apex_fused\"`,\n",
      " |              `\"adamw_anyprecision\"` or `\"adafactor\"`.\n",
      " |          learning_rate (`float`, *optional*, defaults to 5e-5):\n",
      " |              The initial learning rate.\n",
      " |          weight_decay (`float`, *optional*, defaults to 0):\n",
      " |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\n",
      " |          beta1 (`float`, *optional*, defaults to 0.9):\n",
      " |              The beta1 hyperparameter for the adam optimizer or its variants.\n",
      " |          beta2 (`float`, *optional*, defaults to 0.999):\n",
      " |              The beta2 hyperparameter for the adam optimizer or its variants.\n",
      " |          epsilon (`float`, *optional*, defaults to 1e-8):\n",
      " |              The epsilon hyperparameter for the adam optimizer or its variants.\n",
      " |          args (`str`, *optional*):\n",
      " |              Optional arguments that are supplied to AnyPrecisionAdamW (only useful when\n",
      " |              `optim=\"adamw_anyprecision\"`).\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n",
      " |      >>> args.optim\n",
      " |      'adamw_torch'\n",
      " |      ```\n",
      " |\n",
      " |  set_push_to_hub(\n",
      " |      self,\n",
      " |      model_id: str,\n",
      " |      strategy: Union[str, transformers.trainer_utils.HubStrategy] = 'every_save',\n",
      " |      token: Optional[str] = None,\n",
      " |      private_repo: Optional[bool] = None,\n",
      " |      always_push: bool = False,\n",
      " |      revision: Optional[str] = None\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n",
      " |\n",
      " |      <Tip>\n",
      " |\n",
      " |      Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir` will begin a git\n",
      " |      directory synced with the repo (determined by `model_id`) and the content will be pushed each time a save is\n",
      " |      triggered (depending on your `self.save_strategy`). Calling [`~Trainer.save_model`] will also trigger a push.\n",
      " |\n",
      " |      </Tip>\n",
      " |\n",
      " |      Args:\n",
      " |          model_id (`str`):\n",
      " |              The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n",
      " |              which case the model will be pushed in your namespace. Otherwise it should be the whole repository\n",
      " |              name, for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of\n",
      " |              with `\"organization_name/model\"`.\n",
      " |          strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n",
      " |              Defines the scope of what is pushed to the Hub and when. Possible values are:\n",
      " |\n",
      " |              - `\"end\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n",
      " |              draft of a model card when the [`~Trainer.save_model`] method is called.\n",
      " |              - `\"every_save\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`])\n",
      " |                and\n",
      " |              a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n",
      " |              training, and in case the save are very frequent, a new push is only attempted if the previous one is\n",
      " |              finished. A last push is made with the final model at the end of training.\n",
      " |              - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n",
      " |              last-checkpoint, allowing you to resume training easily with\n",
      " |              `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n",
      " |              - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the\n",
      " |                output\n",
      " |              folder (so you will get one checkpoint folder per folder in your final repository)\n",
      " |\n",
      " |          token (`str`, *optional*):\n",
      " |              The token to use to push the model to the Hub. Will default to the token in the cache folder obtained\n",
      " |              with `hf auth login`.\n",
      " |          private_repo (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n",
      " |          always_push (`bool`, *optional*, defaults to `False`):\n",
      " |              Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not\n",
      " |              finished.\n",
      " |          revision (`str`, *optional*):\n",
      " |              The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_push_to_hub(\"me/awesome-model\")\n",
      " |      >>> args.hub_model_id\n",
      " |      'me/awesome-model'\n",
      " |      ```\n",
      " |\n",
      " |  set_save(\n",
      " |      self,\n",
      " |      strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps',\n",
      " |      steps: int = 500,\n",
      " |      total_limit: Optional[int] = None,\n",
      " |      on_each_node: bool = False\n",
      " |  )\n",
      " |      A method that regroups all arguments linked to checkpoint saving.\n",
      " |\n",
      " |      Args:\n",
      " |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n",
      " |              The checkpoint save strategy to adopt during training. Possible values are:\n",
      " |\n",
      " |                  - `\"no\"`: No save is done during training.\n",
      " |                  - `\"epoch\"`: Save is done at the end of each epoch.\n",
      " |                  - `\"steps\"`: Save is done every `save_steps`.\n",
      " |\n",
      " |          steps (`int`, *optional*, defaults to 500):\n",
      " |              Number of updates steps before two checkpoint saves if `strategy=\"steps\"`.\n",
      " |          total_limit (`int`, *optional*):\n",
      " |              If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n",
      " |              `output_dir`.\n",
      " |          on_each_node (`bool`, *optional*, defaults to `False`):\n",
      " |              When doing multi-node distributed training, whether to save models and checkpoints on each node, or\n",
      " |              only on the main one.\n",
      " |\n",
      " |              This should not be activated when the different nodes use the same storage as the files will be saved\n",
      " |              with the same names for each node.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_save(strategy=\"steps\", steps=100)\n",
      " |      >>> args.save_steps\n",
      " |      100\n",
      " |      ```\n",
      " |\n",
      " |  set_testing(\n",
      " |      self,\n",
      " |      batch_size: int = 8,\n",
      " |      loss_only: bool = False,\n",
      " |      jit_mode: bool = False\n",
      " |  )\n",
      " |      A method that regroups all basic arguments linked to testing on a held-out dataset.\n",
      " |\n",
      " |      <Tip>\n",
      " |\n",
      " |      Calling this method will automatically set `self.do_predict` to `True`.\n",
      " |\n",
      " |      </Tip>\n",
      " |\n",
      " |      Args:\n",
      " |          batch_size (`int` *optional*, defaults to 8):\n",
      " |              The batch size per device (GPU/TPU core/CPU...) used for testing.\n",
      " |          loss_only (`bool`, *optional*, defaults to `False`):\n",
      " |              Ignores all outputs except the loss.\n",
      " |          jit_mode (`bool`, *optional*):\n",
      " |              Whether or not to use PyTorch jit trace for inference.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_testing(batch_size=32)\n",
      " |      >>> args.per_device_eval_batch_size\n",
      " |      32\n",
      " |      ```\n",
      " |\n",
      " |  set_training(\n",
      " |      self,\n",
      " |      learning_rate: float = 5e-05,\n",
      " |      batch_size: int = 8,\n",
      " |      weight_decay: float = 0,\n",
      " |      num_epochs: float = 3,\n",
      " |      max_steps: int = -1,\n",
      " |      gradient_accumulation_steps: int = 1,\n",
      " |      seed: int = 42,\n",
      " |      gradient_checkpointing: bool = False\n",
      " |  )\n",
      " |      A method that regroups all basic arguments linked to the training.\n",
      " |\n",
      " |      <Tip>\n",
      " |\n",
      " |      Calling this method will automatically set `self.do_train` to `True`.\n",
      " |\n",
      " |      </Tip>\n",
      " |\n",
      " |      Args:\n",
      " |          learning_rate (`float`, *optional*, defaults to 5e-5):\n",
      " |              The initial learning rate for the optimizer.\n",
      " |          batch_size (`int` *optional*, defaults to 8):\n",
      " |              The batch size per device (GPU/TPU core/CPU...) used for training.\n",
      " |          weight_decay (`float`, *optional*, defaults to 0):\n",
      " |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\n",
      " |              optimizer.\n",
      " |          num_train_epochs(`float`, *optional*, defaults to 3.0):\n",
      " |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n",
      " |              of the last epoch before stopping training).\n",
      " |          max_steps (`int`, *optional*, defaults to -1):\n",
      " |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n",
      " |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n",
      " |              `max_steps` is reached.\n",
      " |          gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n",
      " |              Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
      " |\n",
      " |              <Tip warning={true}>\n",
      " |\n",
      " |              When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\n",
      " |              logging, evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training\n",
      " |              examples.\n",
      " |\n",
      " |              </Tip>\n",
      " |\n",
      " |          seed (`int`, *optional*, defaults to 42):\n",
      " |              Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use\n",
      " |              the [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized\n",
      " |              parameters.\n",
      " |          gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n",
      " |              If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```py\n",
      " |      >>> from transformers import TrainingArguments\n",
      " |\n",
      " |      >>> args = TrainingArguments(\"working_dir\")\n",
      " |      >>> args = args.set_training(learning_rate=1e-4, batch_size=32)\n",
      " |      >>> args.learning_rate\n",
      " |      1e-4\n",
      " |      ```\n",
      " |\n",
      " |  to_dict(self)\n",
      " |      Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n",
      " |      the token values by removing their value.\n",
      " |\n",
      " |  to_json_string(self)\n",
      " |      Serializes this instance to a JSON string.\n",
      " |\n",
      " |  to_sanitized_dict(self) -> dict[str, typing.Any]\n",
      " |      Sanitized serialization to use with TensorBoard’s hparams\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  ddp_timeout_delta\n",
      " |      The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.\n",
      " |\n",
      " |  device\n",
      " |      The device used by this process.\n",
      " |\n",
      " |  eval_batch_size\n",
      " |      The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).\n",
      " |\n",
      " |  local_process_index\n",
      " |      The index of the local process used.\n",
      " |\n",
      " |  n_gpu\n",
      " |      The number of GPUs used by this process.\n",
      " |\n",
      " |      Note:\n",
      " |          This will only be greater than one when you have multiple GPUs available but are not using distributed\n",
      " |          training. For distributed training, it will always be 1.\n",
      " |\n",
      " |  parallel_mode\n",
      " |      The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:\n",
      " |\n",
      " |      - `ParallelMode.NOT_PARALLEL`: no parallelism (CPU or one GPU).\n",
      " |      - `ParallelMode.NOT_DISTRIBUTED`: several GPUs in one single process (uses `torch.nn.DataParallel`).\n",
      " |      - `ParallelMode.DISTRIBUTED`: several GPUs, each having its own process (uses\n",
      " |        `torch.nn.DistributedDataParallel`).\n",
      " |      - `ParallelMode.TPU`: several TPU cores.\n",
      " |\n",
      " |  place_model_on_device\n",
      " |      Can be subclassed and overridden for some specific integrations.\n",
      " |\n",
      " |  process_index\n",
      " |      The index of the current process used.\n",
      " |\n",
      " |  should_log\n",
      " |      Whether or not the current process should produce log.\n",
      " |\n",
      " |  should_save\n",
      " |      Whether or not the current process should write to disk, e.g., to save models and checkpoints.\n",
      " |\n",
      " |  train_batch_size\n",
      " |      The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).\n",
      " |\n",
      " |  world_size\n",
      " |      The number of processes used in parallel.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |\n",
      " |  __annotations__ = {'_n_gpu': <class 'int'>, 'accelerator_config': typi...\n",
      " |\n",
      " |  __dataclass_fields__ = {'_n_gpu': Field(name='_n_gpu',type=<class 'int...\n",
      " |\n",
      " |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      " |\n",
      " |  __hash__ = None\n",
      " |\n",
      " |  __match_args__ = ('output_dir', 'overwrite_output_dir', 'do_train', 'd...\n",
      " |\n",
      " |  accelerator_config = None\n",
      " |\n",
      " |  adafactor = False\n",
      " |\n",
      " |  adam_beta1 = 0.9\n",
      " |\n",
      " |  adam_beta2 = 0.999\n",
      " |\n",
      " |  adam_epsilon = 1e-08\n",
      " |\n",
      " |  auto_find_batch_size = False\n",
      " |\n",
      " |  average_tokens_across_devices = True\n",
      " |\n",
      " |  batch_eval_metrics = False\n",
      " |\n",
      " |  bf16 = False\n",
      " |\n",
      " |  bf16_full_eval = False\n",
      " |\n",
      " |  data_seed = None\n",
      " |\n",
      " |  dataloader_drop_last = False\n",
      " |\n",
      " |  dataloader_num_workers = 0\n",
      " |\n",
      " |  dataloader_persistent_workers = False\n",
      " |\n",
      " |  dataloader_pin_memory = True\n",
      " |\n",
      " |  dataloader_prefetch_factor = None\n",
      " |\n",
      " |  ddp_backend = None\n",
      " |\n",
      " |  ddp_broadcast_buffers = None\n",
      " |\n",
      " |  ddp_bucket_cap_mb = None\n",
      " |\n",
      " |  ddp_find_unused_parameters = None\n",
      " |\n",
      " |  ddp_timeout = 1800\n",
      " |\n",
      " |  debug = ''\n",
      " |\n",
      " |  deepspeed = None\n",
      " |\n",
      " |  default_optim = 'adamw_torch_fused'\n",
      " |\n",
      " |  disable_tqdm = None\n",
      " |\n",
      " |  do_eval = False\n",
      " |\n",
      " |  do_predict = False\n",
      " |\n",
      " |  do_train = False\n",
      " |\n",
      " |  eval_accumulation_steps = None\n",
      " |\n",
      " |  eval_delay = 0\n",
      " |\n",
      " |  eval_do_concat_batches = True\n",
      " |\n",
      " |  eval_on_start = False\n",
      " |\n",
      " |  eval_steps = None\n",
      " |\n",
      " |  eval_strategy = 'no'\n",
      " |\n",
      " |  eval_use_gather_object = False\n",
      " |\n",
      " |  fp16 = False\n",
      " |\n",
      " |  fp16_backend = 'auto'\n",
      " |\n",
      " |  fp16_full_eval = False\n",
      " |\n",
      " |  fp16_opt_level = 'O1'\n",
      " |\n",
      " |  framework = 'pt'\n",
      " |\n",
      " |  fsdp = ''\n",
      " |\n",
      " |  fsdp_config = None\n",
      " |\n",
      " |  fsdp_min_num_params = 0\n",
      " |\n",
      " |  fsdp_transformer_layer_cls_to_wrap = None\n",
      " |\n",
      " |  full_determinism = False\n",
      " |\n",
      " |  gradient_accumulation_steps = 1\n",
      " |\n",
      " |  gradient_checkpointing = False\n",
      " |\n",
      " |  gradient_checkpointing_kwargs = None\n",
      " |\n",
      " |  greater_is_better = None\n",
      " |\n",
      " |  group_by_length = False\n",
      " |\n",
      " |  half_precision_backend = 'auto'\n",
      " |\n",
      " |  hub_always_push = False\n",
      " |\n",
      " |  hub_model_id = None\n",
      " |\n",
      " |  hub_private_repo = None\n",
      " |\n",
      " |  hub_revision = None\n",
      " |\n",
      " |  hub_strategy = 'every_save'\n",
      " |\n",
      " |  hub_token = None\n",
      " |\n",
      " |  ignore_data_skip = False\n",
      " |\n",
      " |  include_inputs_for_metrics = False\n",
      " |\n",
      " |  include_num_input_tokens_seen = False\n",
      " |\n",
      " |  include_tokens_per_second = False\n",
      " |\n",
      " |  is_torch_greater_or_equal_than_2_8 = True\n",
      " |\n",
      " |  jit_mode_eval = False\n",
      " |\n",
      " |  label_names = None\n",
      " |\n",
      " |  label_smoothing_factor = 0.0\n",
      " |\n",
      " |  learning_rate = 5e-05\n",
      " |\n",
      " |  length_column_name = 'length'\n",
      " |\n",
      " |  liger_kernel_config = None\n",
      " |\n",
      " |  load_best_model_at_end = False\n",
      " |\n",
      " |  local_rank = -1\n",
      " |\n",
      " |  log_level = 'passive'\n",
      " |\n",
      " |  log_level_replica = 'warning'\n",
      " |\n",
      " |  log_on_each_node = True\n",
      " |\n",
      " |  logging_dir = None\n",
      " |\n",
      " |  logging_first_step = False\n",
      " |\n",
      " |  logging_nan_inf_filter = True\n",
      " |\n",
      " |  logging_steps = 500\n",
      " |\n",
      " |  logging_strategy = 'steps'\n",
      " |\n",
      " |  lr_scheduler_type = 'linear'\n",
      " |\n",
      " |  max_grad_norm = 1.0\n",
      " |\n",
      " |  max_steps = -1\n",
      " |\n",
      " |  metric_for_best_model = None\n",
      " |\n",
      " |  mp_parameters = ''\n",
      " |\n",
      " |  neftune_noise_alpha = None\n",
      " |\n",
      " |  no_cuda = False\n",
      " |\n",
      " |  num_train_epochs = 3.0\n",
      " |\n",
      " |  optim = 'adamw_torch_fused'\n",
      " |\n",
      " |  optim_args = None\n",
      " |\n",
      " |  optim_target_modules = None\n",
      " |\n",
      " |  output_dir = None\n",
      " |\n",
      " |  overwrite_output_dir = False\n",
      " |\n",
      " |  parallelism_config = None\n",
      " |\n",
      " |  past_index = -1\n",
      " |\n",
      " |  per_device_eval_batch_size = 8\n",
      " |\n",
      " |  per_device_train_batch_size = 8\n",
      " |\n",
      " |  per_gpu_eval_batch_size = None\n",
      " |\n",
      " |  per_gpu_train_batch_size = None\n",
      " |\n",
      " |  prediction_loss_only = False\n",
      " |\n",
      " |  push_to_hub = False\n",
      " |\n",
      " |  push_to_hub_model_id = None\n",
      " |\n",
      " |  push_to_hub_organization = None\n",
      " |\n",
      " |  push_to_hub_token = None\n",
      " |\n",
      " |  ray_scope = 'last'\n",
      " |\n",
      " |  remove_unused_columns = True\n",
      " |\n",
      " |  report_to = None\n",
      " |\n",
      " |  restore_callback_states_from_checkpoint = False\n",
      " |\n",
      " |  resume_from_checkpoint = None\n",
      " |\n",
      " |  run_name = None\n",
      " |\n",
      " |  save_on_each_node = False\n",
      " |\n",
      " |  save_only_model = False\n",
      " |\n",
      " |  save_safetensors = True\n",
      " |\n",
      " |  save_steps = 500\n",
      " |\n",
      " |  save_strategy = 'steps'\n",
      " |\n",
      " |  save_total_limit = None\n",
      " |\n",
      " |  seed = 42\n",
      " |\n",
      " |  skip_memory_metrics = True\n",
      " |\n",
      " |  tf32 = None\n",
      " |\n",
      " |  torch_compile = False\n",
      " |\n",
      " |  torch_compile_backend = None\n",
      " |\n",
      " |  torch_compile_mode = None\n",
      " |\n",
      " |  torch_empty_cache_steps = None\n",
      " |\n",
      " |  torchdynamo = None\n",
      " |\n",
      " |  tpu_metrics_debug = False\n",
      " |\n",
      " |  tpu_num_cores = None\n",
      " |\n",
      " |  use_cpu = False\n",
      " |\n",
      " |  use_ipex = False\n",
      " |\n",
      " |  use_legacy_prediction_loop = False\n",
      " |\n",
      " |  use_liger_kernel = False\n",
      " |\n",
      " |  use_mps_device = False\n",
      " |\n",
      " |  warmup_ratio = 0.0\n",
      " |\n",
      " |  warmup_steps = 0\n",
      " |\n",
      " |  weight_decay = 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "help(TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3531ae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glove-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
